:scrollbar:
:data-uri:
:toc2:
:linkattrs:


== Introduction to Lab

:numbered:


== Access Lab Environment

Your CloudForms Customization lab environment contains the following:

* Student workstation
* CloudForms appliance
* Red Hat Virtualization Manager
* Red Hat Enterprise Linux + KVM host
* VMware vSphere + 2 ESXi hosts
* Red Hat Satellite
* Ansible Tower by Red Hat
* Infoblox IPAM appliance

[IMPORTANT]
The CloudForms appliance is fully configured with all of the above providers.

=== Access Lab Workstation

. Use SSH to access the lab workstation using your private SSH key and OPENTLC SSO credentials:
.. For macOS and Linux users, execute the following:
+
[source,text]
----
$ ssh -i /path/to/your/ssh-key your-opentlc-sso-login@workstation-$GUID.rhpds.opentlc.com
----
+
[NOTE]
Replace `$GUID` with your lab environment GUID provided in the provisioning email you received when ordering.  Replace `your-opentlc-sso-login` with your OPENTLC SSO user name.

.. For Windows users, consult link:https://www.opentlc.com/ssh.html[this page^] for documentation on using PuTTY.
+
[IMPORTANT]
To avoid problems when using SSH to access your OPENTLC lab hosts, always use the FQDN host name and not the IP or Ravello DNS entry.

. After logging in, make sure you can become `root` on the lab workstation:
+
[source,text]
----
$ sudo -i
#
----

=== Access Red Hat Enterprise Virtualization Console with macOS

[IMPORTANT]
Red Hat Enterprise Virtualization does not support macOS for VM console access. See link:https://access.redhat.com/solutions/93613[this page^] for unsupported steps on obtaining console access for macOS.

=== Verify Provider Authentication Status

When the CloudForms appliance and providers are down for any period of time (stopped overnight or initially provisioned in the lab environment) they lose connection to each other. CloudForms does not immediately re-establish connectivity. Every time you start your lab environment you must re-establish the communication between CloudForms and your providers. The appliance does attempt to reconnect at some point, but it may take hours, so it is best to follow the procedure below to get things running sooner.

[WARNING]
If you do not do this every time you start your environment, some of the labs will hang and you will have to start over.

. Log in to your CloudForms appliance at `https://cf-$GUID.rhpds.opentlc.com` as the `admin` user with the `r3dh4t1!` password.
+
[NOTE]
Replace `$GUID` with the GUID provided to you in the email you received from the OPENTLC lab system.

. Navigate to *Compute -> Infrastructure -> Providers*.

. Make sure all of your providers have image:images/greencheck.png[] in the status quadrant as shown here:
+
image::images/goodproviders.png[]

. If your providers do not have a image:images/greencheck.png[], you must re-check authentication status:
.. Click the provider and select *Authentication -> Re-check Authentication Status*:
+
image::images/recheck.png[]

. Select *Compute -> Infrastructure -> Providers* repeatedly until the provider's status is image:images/greencheck.png[].

. Repeat this procedure for both Red Hat Virtualization and vSphere providers.
+
[IMPORTANT]
Verify each provider's status following _every_ lab environment shutdown.

== Lab 1 Instructions

. Log on to your workstation machine as previously described.

. From there, log in to the *ansible1.example.com* host
+
----
$ ssh root@ansible1.example.com
----

. Clone the Tech Exchange repository from https://github.com/redhat-gpe/rhte-na.git
+
----
$ git clone https://github.com/redhat-gpe/rhte-na.git
----

. Change directory to rhte-na/hybrid_cloud_management/labs/lab1
+
----
$ cd /root/rhte_na/hybrid_cloud_management/labs/lab1/source
----

. View *playbook1.yml* within this directory. The playbook contains ansible code to create and configure a web server. This includes opening the necessary ports on the firewall and ensuring the latest packages be installed.
+
If you notice, the last piece of the playbook is to test that you get a *200* response to the webserver from a different machine.
+
The first play in the playbook runs on the `workstation.example.com` host while the second runs from `ansible1.example.com`.

. Run the *playbook1.yml* playbook.
+
----
$ ansible-playbook -i inventory playbook1.yml
----
+
Analyze the run log and view the output.
+
[NOTE]
This output is not verbose. For standard output open the playbook add *--verbose* to the end of your ansible-playbook command.

. Run the *cleanup.yml* playbook.
+
----
$ ansible-playbook -i inventory cleanup.yml
----

. Switch to the roles directory under the current directory
+
----
$ cd roles
----

. Browse the roles directory, specifically the webserver role directory.
+
The roles directory has the webserver role as well as a requirements.yml file. We will not be using the requirements.yml file in the main portion of the lab, though for those who have experience you may want to try *insert what lab step this will be* in the *Stretch Goals* section of the lab.
+
The webserver role provides an example of an ansible role layout. In the tasks folder is a task list for the role with the name *main.yml*. 
+
*TODO: Add more description of what is contained in the role*

. Open and view playbook2.
+
----
$ cd /root/rhte-na/hybrid_cloud_management/labs/lab1
$ less playbook2.yml
----
+
This playbook is far simpler than *playbook1.yml* but performs the same function.
+
Notice how the webserver role is included.

. Run the *playbook2.yml* playbook.
+
----
$ ansible-playbook -i inventory playbook2.yml
----

. Run the same step a second time. On the second run though you should notice that all of the steps are labeled as **completed**.
+
----
$ ansible-playbook -i inventory playbook2.yml
----

. Modify the playbook to set a variable for the role to use.
+
Add the following lines:
+
[subs=+quotes]
----
- hosts
  *vars:*
     *body_content: "This page is now changed"*
  roles:
----

. Run the *playbook2.yml* playbook again and observe the change that takes place.
+
----
$ ansible-playbook -i inventory playbook2.yml
----

. Create a new playbook named *playbook3.yml* to create a file. Use the previous playbooks as well as the included module documentation for reference. The file should be called *hello_ansible.txt* located in */home/consultant*. The content of the file should be *"Hello World, from Ansible." This playbook should be able to run idempotently. It should also run against the host: *workstation.example.com*. 
